{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lexele/Library/Python/3.11/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# There were issues with my certificate\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Data Processing\n",
    "Here we load the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "# batch_size = 4 # In the powerpoint slides it is 32?\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #functions to show an image\n",
    "# def imshow(img):\n",
    "# \timg = img / 2 + 0.5 # unnormalize\n",
    "# \tnpimg = img.numpy()\n",
    "# \tplt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "# \tplt.show()\n",
    "\n",
    "\n",
    "# #get random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# # print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "\tif torch.cuda.is_available():\n",
    "\t\treturn torch.device ('cuda' )\n",
    "\telif torch.backends.mps.is_available():\n",
    "\t\treturn torch.device ('mps')\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')\n",
    "\t\n",
    "device = get_device()\t\n",
    "\n",
    "\n",
    "def preprocess_data(transformations, batch_size):\n",
    "\t\"\"\"\n",
    "\tThis function creates dataloaders with model-specific transformations for data preprocessing\n",
    "\t\"\"\"\n",
    "\ttrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformations)\t\n",
    "\ttrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\ttest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
    "\ttest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\t# Should we return train_set and test_loader too?\n",
    "\treturn train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our CNNs\n",
    "***\n",
    "We will be experimenting with various pre-trained Deep CNNs - specifically ResNet and VGG architectures.\n",
    "\n",
    "We will be using 2 different versions of Resnet and 2 different versions of VGG.\n",
    "\n",
    "VGG: We will be using models `vgg11` , `vgg11_bn` (with batch normalization), `vgg19`, `vgg19_bn`\n",
    "\n",
    "RESNET: We will be using `resnet18` (ResNet-18) and `resnet152` (ResNet-152)\n",
    "\n",
    "** apply its specific preprocessing transformations to  CIFAR-10 dataset when creating DataLoader objects. **\n",
    "\n",
    "For each model we use, we will:\n",
    "1. Import, insantiate, and load it with default, pretrained weights\n",
    "2. Get the correct preprocessing transformations for that specific model\n",
    "3. Apply those transformations to our CIFAR10 dataset\n",
    "4. Modify the classifer part of the model for our 10-class classification task.\n",
    "-> this means that we need to freeze the ConvLayer(s) parts of the network, and modify the last couple layers -- follow Lecture 9 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "            Conv2d-3         [32, 64, 224, 224]          36,928\n",
      "              ReLU-4         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [32, 64, 112, 112]               0\n",
      "            Conv2d-6        [32, 128, 112, 112]          73,856\n",
      "              ReLU-7        [32, 128, 112, 112]               0\n",
      "            Conv2d-8        [32, 128, 112, 112]         147,584\n",
      "              ReLU-9        [32, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [32, 128, 56, 56]               0\n",
      "           Conv2d-11          [32, 256, 56, 56]         295,168\n",
      "             ReLU-12          [32, 256, 56, 56]               0\n",
      "           Conv2d-13          [32, 256, 56, 56]         590,080\n",
      "             ReLU-14          [32, 256, 56, 56]               0\n",
      "           Conv2d-15          [32, 256, 56, 56]         590,080\n",
      "             ReLU-16          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [32, 256, 28, 28]               0\n",
      "           Conv2d-18          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [32, 512, 28, 28]               0\n",
      "           Conv2d-20          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [32, 512, 28, 28]               0\n",
      "           Conv2d-22          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [32, 512, 14, 14]               0\n",
      "           Conv2d-25          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [32, 512, 14, 14]               0\n",
      "           Conv2d-27          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [32, 512, 14, 14]               0\n",
      "           Conv2d-29          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [32, 512, 7, 7]               0\n",
      "           Linear-33                 [32, 4096]     102,764,544\n",
      "             ReLU-34                 [32, 4096]               0\n",
      "          Dropout-35                 [32, 4096]               0\n",
      "           Linear-36                 [32, 4096]      16,781,312\n",
      "             ReLU-37                 [32, 4096]               0\n",
      "          Dropout-38                 [32, 4096]               0\n",
      "           Linear-39                 [32, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 7000.99\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 7547.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16().to(device)\n",
    "\n",
    "\n",
    "# summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "def better_summary(model, input_size, batch_size=32):\n",
    "\t\"\"\"\n",
    "\tA better summary function that ensures device consistency between weights used for the VGG model\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Get the device the model is on\n",
    "\tmodel_on_cpu = model.to('cpu')\n",
    "\tsummary(model_on_cpu, input_size, batch_size=batch_size)\n",
    "\t# Move model back to original device\n",
    "\tmodel.to(device)\n",
    "\n",
    "better_summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [32, 64, 112, 112]               0\n",
      "            Conv2d-4        [32, 128, 112, 112]          73,856\n",
      "              ReLU-5        [32, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [32, 128, 56, 56]               0\n",
      "            Conv2d-7          [32, 256, 56, 56]         295,168\n",
      "              ReLU-8          [32, 256, 56, 56]               0\n",
      "            Conv2d-9          [32, 256, 56, 56]         590,080\n",
      "             ReLU-10          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [32, 256, 28, 28]               0\n",
      "           Conv2d-12          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [32, 512, 28, 28]               0\n",
      "           Conv2d-14          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [32, 512, 14, 14]               0\n",
      "           Conv2d-17          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [32, 512, 14, 14]               0\n",
      "           Conv2d-19          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [32, 512, 7, 7]               0\n",
      "           Linear-23                 [32, 4096]     102,764,544\n",
      "             ReLU-24                 [32, 4096]               0\n",
      "          Dropout-25                 [32, 4096]               0\n",
      "           Linear-26                 [32, 4096]      16,781,312\n",
      "             ReLU-27                 [32, 4096]               0\n",
      "          Dropout-28                 [32, 4096]               0\n",
      "           Linear-29                 [32, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 132,863,336\n",
      "Trainable params: 132,863,336\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 4011.99\n",
      "Params size (MB): 506.83\n",
      "Estimated Total Size (MB): 4537.20\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [32, 64, 112, 112]               0\n",
      "            Conv2d-4        [32, 128, 112, 112]          73,856\n",
      "              ReLU-5        [32, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [32, 128, 56, 56]               0\n",
      "            Conv2d-7          [32, 256, 56, 56]         295,168\n",
      "              ReLU-8          [32, 256, 56, 56]               0\n",
      "            Conv2d-9          [32, 256, 56, 56]         590,080\n",
      "             ReLU-10          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [32, 256, 28, 28]               0\n",
      "           Conv2d-12          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [32, 512, 28, 28]               0\n",
      "           Conv2d-14          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [32, 512, 14, 14]               0\n",
      "           Conv2d-17          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [32, 512, 14, 14]               0\n",
      "           Conv2d-19          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [32, 512, 1, 1]               0\n",
      "          Flatten-23                  [32, 512]               0\n",
      "           Linear-24                  [32, 128]          65,664\n",
      "           Linear-25                    [32, 1]             129\n",
      "================================================================\n",
      "Total params: 9,286,273\n",
      "Trainable params: 65,793\n",
      "Non-trainable params: 9,220,480\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 3999.91\n",
      "Params size (MB): 35.42\n",
      "Estimated Total Size (MB): 4053.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, resnet152, ResNet18_Weights, ResNet152_Weights\n",
    "from torchvision.models import vgg11, vgg11_bn, vgg19, vgg19_bn, VGG11_Weights, VGG19_Weights\n",
    "\n",
    "batch_size = 32\n",
    "# Load ResNet models with pretrained weights\n",
    "resnet18_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "resnet152_model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "# Load VGG models with pretrained weights for transfer learning\n",
    "vgg11_model = vgg11(weights=VGG11_Weights.DEFAULT)\n",
    "vgg19_model = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "# Get transformations for every model\n",
    "resnet18_transforms = ResNet18_Weights.DEFAULT.transforms()\n",
    "resnet152_transforms = ResNet152_Weights.DEFAULT.transforms()\n",
    "vgg11_transforms = VGG11_Weights.DEFAULT.transforms()\n",
    "vgg19_transforms = VGG19_Weights.DEFAULT.transforms()\n",
    "\n",
    "models = [vgg11_model]\n",
    "\n",
    "\n",
    "# Create dataloaders for each model with its specific transforms\n",
    "resnet18_train_loader, resnet18_test_loader = preprocess_data(resnet18_transforms, batch_size=batch_size)\n",
    "resnet152_train_loader, resnet152_test_loader = preprocess_data(resnet152_transforms, batch_size=batch_size)\n",
    "vgg11_train_loader, vgg11_test_loader = preprocess_data(vgg11_transforms, batch_size=batch_size)\n",
    "vgg19_train_loader, vgg19_test_loader = preprocess_data(vgg19_transforms, batch_size=batch_size) \n",
    "list_of_transformations = [resnet18_transforms, resnet152_transforms, vgg11_transforms, vgg19_transforms]\n",
    "\n",
    "for model in models:\n",
    "\tbetter_summary(model.to(device), (3, 224, 224))\n",
    "\t# print(model)\n",
    "\tfor param in model.features.parameters():\n",
    "\t\tparam.requires_grad = False # Freezing these layers\n",
    "\n",
    "\tmodel.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "\tmodel.classifier = nn.Sequential(nn.Flatten(),\n",
    "\t\t\t\t\t\t\t\t  \tnn.Linear(512, 128),\n",
    "\t\t\t\t\t\t\t\t \tnn.Linear(128, 1))\n",
    "\t\n",
    "\tbetter_summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Experimentation and Transfer Learning\n",
    "\n",
    "Doing transfer learning on ResNet is slightly different from VGG (the layers don't have the same names), so we print the networks to know which layers to freeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 of 5\n",
      "Running epoch 2 of 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m train_epoch_losses \u001b[38;5;241m=\u001b[39m [train_batch(model, optimizer, loss_fn, x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mto(device)) \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m vgg11_train_loader]\n\u001b[1;32m     29\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_epoch_losses)\n\u001b[0;32m---> 31\u001b[0m train_epoch_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvgg11_train_loader\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m train_epoch_accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_epoch_accuracies)\n\u001b[1;32m     34\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n",
      "Cell \u001b[0;32mIn[11], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m train_epoch_losses \u001b[38;5;241m=\u001b[39m [train_batch(model, optimizer, loss_fn, x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mto(device)) \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m vgg11_train_loader]\n\u001b[1;32m     29\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_epoch_losses)\n\u001b[0;32m---> 31\u001b[0m train_epoch_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvgg11_train_loader\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m train_epoch_accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_epoch_accuracies)\n\u001b[1;32m     34\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_epoch_loss)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/_presets.py:61\u001b[0m, in \u001b[0;36mImageClassification.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcenter_crop(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_size)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, Tensor):\n\u001b[0;32m---> 61\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpil_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconvert_image_dtype(img, torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     63\u001b[0m img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(img, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/functional.py:207\u001b[0m, in \u001b[0;36mpil_to_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mas_tensor(nppic)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(np\u001b[38;5;241m.\u001b[39marray(pic, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    208\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_batch(model, opt, loss_fn, x, y):\n",
    "\tmodel.train()\n",
    "\t\n",
    "\topt.zero_grad()\n",
    "\tbatch_loss = loss_fn(model(x), y) # Loss\n",
    "\tbatch_loss.backward() # Compute gradients\n",
    "\topt.step() # Make a GD step\n",
    "\n",
    "\treturn batch_loss.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(model, x, y,):\n",
    "\tmodel.eval()\n",
    "\tprediction = model(x)\n",
    "\targmaxes = prediction.argmax(dim=1)\n",
    "\ts = torch.sum((argmaxes == y).float()/len(y))\n",
    "\treturn s.cpu().numpy()\n",
    "\n",
    "\n",
    "train_losses, train_accuracies, n_epochs = [], [], 5\n",
    "for epoch in range(n_epochs):\n",
    "\tprint(f\"Running epoch {epoch + 1} of {n_epochs}\")\n",
    "\ttrain_epoch_losses, train_accuracies = [], []\n",
    "\n",
    "\ttrain_epoch_losses = [train_batch(model, optimizer, loss_fn, x=x.to(device), y=y.to(device)) for x, y in vgg11_train_loader]\n",
    "\ttrain_epoch_loss = np.mean(train_epoch_losses)\n",
    "\n",
    "\ttrain_epoch_accuracies = [accuracy(model, x=x.to(device), y=y.to(device)) for x, y in vgg11_train_loader]\n",
    "\ttrain_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "\n",
    "\ttrain_losses.append(train_epoch_loss)\n",
    "\ttrain_accuracies.append(train_epoch_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
