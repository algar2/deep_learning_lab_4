{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lexele/Library/Python/3.11/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# There were issues with my certificate\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Data Processing\n",
    "Here we load the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "# batch_size = 4 # In the powerpoint slides it is 32?\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #functions to show an image\n",
    "# def imshow(img):\n",
    "# \timg = img / 2 + 0.5 # unnormalize\n",
    "# \tnpimg = img.numpy()\n",
    "# \tplt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "# \tplt.show()\n",
    "\n",
    "\n",
    "# #get random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# # print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(transformations, batch_size):\n",
    "\t\"\"\"\n",
    "\tThis function creates dataloaders with model-specific transformations for data preprocessing\n",
    "\t\"\"\"\n",
    "\ttrain_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformations)\n",
    "\ttrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\ttest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
    "\ttest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\t# Should we return train_set and test_loader too?\n",
    "\treturn train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our CNNs\n",
    "***\n",
    "We will be experimenting with various pre-trained Deep CNNs - specifically ResNet and VGG architectures.\n",
    "\n",
    "We will be using 2 different versions of Resnet and 2 different versions of VGG.\n",
    "\n",
    "VGG: We will be using models `vgg11` , `vgg11_bn` (with batch normalization), `vgg19`, `vgg19_bn`\n",
    "\n",
    "RESNET: We will be using `resnet18` (ResNet-18) and `resnet152` (ResNet-152)\n",
    "\n",
    "** apply its specific preprocessing transformations to  CIFAR-10 dataset when creating DataLoader objects. **\n",
    "\n",
    "For each model we use, we will:\n",
    "1. Import, insantiate, and load it with default, pretrained weights\n",
    "2. Get the correct preprocessing transformations for that specific model\n",
    "3. Apply those transformations to our CIFAR10 dataset\n",
    "4. Modify the classifer part of the model for our 10-class classification task.\n",
    "-> this means that we need to freeze the ConvLayer(s) parts of the network, and modify the last couple layers -- follow Lecture 9 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "            Conv2d-3         [32, 64, 224, 224]          36,928\n",
      "              ReLU-4         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [32, 64, 112, 112]               0\n",
      "            Conv2d-6        [32, 128, 112, 112]          73,856\n",
      "              ReLU-7        [32, 128, 112, 112]               0\n",
      "            Conv2d-8        [32, 128, 112, 112]         147,584\n",
      "              ReLU-9        [32, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [32, 128, 56, 56]               0\n",
      "           Conv2d-11          [32, 256, 56, 56]         295,168\n",
      "             ReLU-12          [32, 256, 56, 56]               0\n",
      "           Conv2d-13          [32, 256, 56, 56]         590,080\n",
      "             ReLU-14          [32, 256, 56, 56]               0\n",
      "           Conv2d-15          [32, 256, 56, 56]         590,080\n",
      "             ReLU-16          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [32, 256, 28, 28]               0\n",
      "           Conv2d-18          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [32, 512, 28, 28]               0\n",
      "           Conv2d-20          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [32, 512, 28, 28]               0\n",
      "           Conv2d-22          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [32, 512, 14, 14]               0\n",
      "           Conv2d-25          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [32, 512, 14, 14]               0\n",
      "           Conv2d-27          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [32, 512, 14, 14]               0\n",
      "           Conv2d-29          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [32, 512, 7, 7]               0\n",
      "           Linear-33                 [32, 4096]     102,764,544\n",
      "             ReLU-34                 [32, 4096]               0\n",
      "          Dropout-35                 [32, 4096]               0\n",
      "           Linear-36                 [32, 4096]      16,781,312\n",
      "             ReLU-37                 [32, 4096]               0\n",
      "          Dropout-38                 [32, 4096]               0\n",
      "           Linear-39                 [32, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 7000.99\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 7547.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "\tif torch.cuda.is_available():\n",
    "\t\treturn torch. device ('cuda' )\n",
    "\telif torch.backends.mps.is_available():\n",
    "\t\treturn torch.device ('mps')\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')\n",
    "\t\n",
    "device = get_device()\t\n",
    "model = models.vgg16().to(device)\n",
    "\n",
    "\n",
    "# summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "def better_summary(model, input_size, batch_size=32):\n",
    "\t\"\"\"\n",
    "\tA better summary function that ensures device consistency between weights used for the VGG model\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Get the device the model is on\n",
    "\tmodel_on_cpu = model.to('cpu')\n",
    "\tsummary(model_on_cpu, input_size, batch_size=batch_size)\n",
    "\t# Move model back to original device\n",
    "\tmodel.to(device)\n",
    "\n",
    "better_summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [32, 64, 112, 112]               0\n",
      "            Conv2d-4        [32, 128, 112, 112]          73,856\n",
      "              ReLU-5        [32, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [32, 128, 56, 56]               0\n",
      "            Conv2d-7          [32, 256, 56, 56]         295,168\n",
      "              ReLU-8          [32, 256, 56, 56]               0\n",
      "            Conv2d-9          [32, 256, 56, 56]         590,080\n",
      "             ReLU-10          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [32, 256, 28, 28]               0\n",
      "           Conv2d-12          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [32, 512, 28, 28]               0\n",
      "           Conv2d-14          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [32, 512, 14, 14]               0\n",
      "           Conv2d-17          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [32, 512, 14, 14]               0\n",
      "           Conv2d-19          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [32, 512, 7, 7]               0\n",
      "           Linear-23                 [32, 4096]     102,764,544\n",
      "             ReLU-24                 [32, 4096]               0\n",
      "          Dropout-25                 [32, 4096]               0\n",
      "           Linear-26                 [32, 4096]      16,781,312\n",
      "             ReLU-27                 [32, 4096]               0\n",
      "          Dropout-28                 [32, 4096]               0\n",
      "           Linear-29                 [32, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 132,863,336\n",
      "Trainable params: 132,863,336\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 4011.99\n",
      "Params size (MB): 506.83\n",
      "Estimated Total Size (MB): 4537.20\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [32, 64, 112, 112]               0\n",
      "            Conv2d-4        [32, 128, 112, 112]          73,856\n",
      "              ReLU-5        [32, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [32, 128, 56, 56]               0\n",
      "            Conv2d-7          [32, 256, 56, 56]         295,168\n",
      "              ReLU-8          [32, 256, 56, 56]               0\n",
      "            Conv2d-9          [32, 256, 56, 56]         590,080\n",
      "             ReLU-10          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [32, 256, 28, 28]               0\n",
      "           Conv2d-12          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [32, 512, 28, 28]               0\n",
      "           Conv2d-14          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [32, 512, 14, 14]               0\n",
      "           Conv2d-17          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [32, 512, 14, 14]               0\n",
      "           Conv2d-19          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [32, 512, 1, 1]               0\n",
      "          Flatten-23                  [32, 512]               0\n",
      "           Linear-24                  [32, 128]          65,664\n",
      "           Linear-25                    [32, 1]             129\n",
      "================================================================\n",
      "Total params: 9,286,273\n",
      "Trainable params: 65,793\n",
      "Non-trainable params: 9,220,480\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 3999.91\n",
      "Params size (MB): 35.42\n",
      "Estimated Total Size (MB): 4053.71\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, resnet152, ResNet18_Weights, ResNet152_Weights\n",
    "from torchvision.models import vgg11, vgg11_bn, vgg19, vgg19_bn, VGG11_Weights, VGG19_Weights\n",
    "\n",
    "batch_size = 32\n",
    "# Load ResNet models with pretrained weights\n",
    "resnet18_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "resnet152_model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "# Load VGG models with pretrained weights for transfer learning\n",
    "vgg11_model = vgg11(weights=VGG11_Weights.DEFAULT)\n",
    "vgg19_model = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "# Get transformations for every model\n",
    "resnet18_transforms = ResNet18_Weights.DEFAULT.transforms()\n",
    "resnet152_transforms = ResNet152_Weights.DEFAULT.transforms()\n",
    "vgg11_transforms = VGG11_Weights.DEFAULT.transforms()\n",
    "vgg19_transforms = VGG19_Weights.DEFAULT.transforms()\n",
    "\n",
    "models = [vgg11_model]\n",
    "\n",
    "\n",
    "# Create dataloaders for each model with its specific transforms\n",
    "resnet18_train_loader, resnet18_test_loader = preprocess_data(resnet18_transforms, batch_size=batch_size)\n",
    "resnet152_train_loader, resnet152_test_loader = preprocess_data(resnet152_transforms, batch_size=batch_size)\n",
    "vgg11_train_loader, vgg11_test_loader = preprocess_data(vgg11_transforms, batch_size=batch_size)\n",
    "vgg19_train_loader, vgg19_test_loader = preprocess_data(vgg19_transforms, batch_size=batch_size) \n",
    "list_of_transformations = [resnet18_transforms, resnet152_transforms, vgg11_transforms, vgg19_transforms]\n",
    "\n",
    "for model in models:\n",
    "\tbetter_summary(model.to(device), (3, 224, 224))\n",
    "\t# print(model)\n",
    "\tfor param in model.features.parameters():\n",
    "\t\tparam.requires_grad = False\n",
    "\n",
    "\tmodel.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "\tmodel.classifier = nn.Sequential(nn.Flatten(),\n",
    "\t\t\t\t\t\t\t\t  \tnn.Linear(512, 128),\n",
    "\t\t\t\t\t\t\t\t \tnn.Linear(128, 1))\n",
    "\t\n",
    "\tbetter_summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a MLP\n",
    "This is where the classifier is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Experimentation and Transfer Learning\n",
    "\n",
    "Doing transfer learning on ResNet is slightly different from VGG (the layers don't have the same names), so we print the networks to know which layers to freeze."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
