{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.nn import ReLU\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# There were issues with my certificate\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Data Processing\n",
    "Here we load the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "# batch_size = 4 # In the powerpoint slides it is 32?\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #functions to show an image\n",
    "# def imshow(img):\n",
    "# \timg = img / 2 + 0.5 # unnormalize\n",
    "# \tnpimg = img.numpy()\n",
    "# \tplt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "# \tplt.show()\n",
    "\n",
    "\n",
    "# #get random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# # print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "\tif torch.cuda.is_available():\n",
    "\t\treturn torch.device ('cuda' )\n",
    "\telif torch.backends.mps.is_available():\n",
    "\t\treturn torch.device ('mps')\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')\n",
    "\t\n",
    "device = get_device()\t\n",
    "\n",
    "\n",
    "def preprocess_data(transformations, batch_size, subset_size=10000):\n",
    "\t\"\"\"\n",
    "\tThis function creates dataloaders with model-specific transformations for data preprocessing\n",
    "\t\"\"\"\n",
    "\t# Load the full training dataset\n",
    "\tfull_train_set = torchvision.datasets.CIFAR10(\n",
    "\t\troot='./data', \n",
    "\t\ttrain=True, \n",
    "\t\tdownload=True, \n",
    "\t\ttransform=transformations\n",
    "\t)\t\n",
    "\n",
    "\t# Creating a subset of the training data\n",
    "\tif subset_size and subset_size < len(full_train_set):\n",
    "\t\t# Creates a random subset\n",
    "\t\tindices = torch.randperm(len(full_train_set))[:subset_size]\n",
    "\t\ttrain_dataset = Subset(full_train_set,indices)\n",
    "\telse:\n",
    "\t\ttrain_dataset = full_train_set\n",
    "\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\ttest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
    "\ttest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\t# Should we return train_set and test_loader too?\n",
    "\treturn train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our CNNs\n",
    "***\n",
    "We will be experimenting with various pre-trained Deep CNNs - specifically ResNet and VGG architectures.\n",
    "\n",
    "We will be using 2 different versions of Resnet and 2 different versions of VGG. We use weights `ResNet_Weights.DEFAULT` for our pretrained model.\n",
    "\n",
    "VGG: We will be using models `vgg11` , `vgg11_bn` (with batch normalization), `vgg19`, `vgg19_bn`\n",
    "\n",
    "RESNET: We will be using `resnet18` (ResNet-18) and `resnet152` (ResNet-152)\n",
    "\n",
    "** apply its specific preprocessing transformations to  CIFAR-10 dataset when creating DataLoader objects. **\n",
    "\n",
    "For each model we use, we will:\n",
    "1. Import, insantiate, and load it with default, pretrained weights\n",
    "2. Get the correct preprocessing transformations for that specific model\n",
    "3. Apply those transformations to our CIFAR10 dataset\n",
    "4. Modify the classifer part of the model for our 10-class classification task.\n",
    "-> this means that we need to freeze the ConvLayer(s) parts of the network, and modify the last couple layers -- follow Lecture 9 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16().to(device)\n",
    "\n",
    "\n",
    "# summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "def better_summary(model, input_size, batch_size=32):\n",
    "\t\"\"\"\n",
    "\tA better summary function that ensures device consistency between weights used for the VGG model\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Get the device the model is on\n",
    "\tmodel_on_cpu = model.to('cpu')\n",
    "\tsummary(model_on_cpu, input_size, batch_size=batch_size)\n",
    "\t# Move model back to original device\n",
    "\tmodel.to(device)\n",
    "\n",
    "# better_summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, resnet152, ResNet18_Weights, ResNet152_Weights\n",
    "from torchvision.models import vgg11, vgg11_bn, vgg19, vgg19_bn, VGG11_Weights, VGG19_Weights\n",
    "\n",
    "batch_size = 32\n",
    "# Load ResNet models with pretrained weights\n",
    "resnet18_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "resnet152_model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "# Load VGG models with pretrained weights for transfer learning\n",
    "vgg11_model = vgg11(weights=VGG11_Weights.DEFAULT)\n",
    "vgg19_model = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "# Get transformations for every model\n",
    "resnet18_transforms = ResNet18_Weights.DEFAULT.transforms()\n",
    "resnet152_transforms = ResNet152_Weights.DEFAULT.transforms()\n",
    "vgg11_transforms = VGG11_Weights.DEFAULT.transforms()\n",
    "vgg19_transforms = VGG19_Weights.DEFAULT.transforms()\n",
    "\n",
    "\n",
    "# Create dataloaders for each model with its specific transforms\n",
    "resnet18_train_loader, resnet18_test_loader = preprocess_data(resnet18_transforms, batch_size=batch_size)\n",
    "resnet152_train_loader, resnet152_test_loader = preprocess_data(resnet152_transforms, batch_size=batch_size)\n",
    "vgg11_train_loader, vgg11_test_loader = preprocess_data(vgg11_transforms, batch_size=batch_size)\n",
    "vgg19_train_loader, vgg19_test_loader = preprocess_data(vgg19_transforms, batch_size=batch_size) \n",
    "list_of_transformations = [resnet18_transforms, resnet152_transforms, vgg11_transforms, vgg19_transforms]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Experimentation and Transfer Learning\n",
    "\n",
    "\n",
    "Doing transfer learning on ResNet is slightly different from VGG (the layers don't have the same names), so we print the networks to know which layers to freeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_batch(model, opt, loss_fn, x, y):\n",
    "\tmodel.train()\n",
    "\t\n",
    "\topt.zero_grad()\n",
    "\tbatch_loss = loss_fn(model(x.to(device)), y.to(device)) # Loss\n",
    "\tbatch_loss.backward() # Compute gradients\n",
    "\topt.step() # Make a GD step\n",
    "\n",
    "\treturn batch_loss.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(model, x, y,):\n",
    "\tmodel.eval()\n",
    "\tprediction = model(x.to(device))\n",
    "\targmaxes = prediction.argmax(dim=1)\n",
    "\ts = torch.sum((argmaxes == y).float()/len(y))\n",
    "\treturn s.cpu().numpy()\n",
    "\n",
    "def perform_training(model, train_dataloader):\n",
    "\t\"\"\"\n",
    "\tTrain the model and evaluate \n",
    "\t\"\"\"\n",
    "\ttrain_losses, train_accuracies, n_epochs = [], [], 5\n",
    "\n",
    "\tstart_time = timeit.default_timer()\n",
    "\tfor epoch in range(n_epochs):\n",
    "\t\tprint(f\"Running epoch {epoch + 1} of {n_epochs}\")\n",
    "\n",
    "\t\ttrain_epoch_losses, train_epoch_accuracies = [], []\n",
    "\t\t\n",
    "\t\tfor x, y in train_dataloader:\n",
    "\t\t\tx,y = x.to(device), y.to(device)\n",
    "\t\t\ttrain_epoch_losses = train_batch(model, optimizer, loss_fn, x, y)\n",
    "\t\t\ttrain_epoch_accuracies = accuracy(model, x, y)\n",
    "\n",
    "\t\t# train_epoch_loss = \n",
    "\n",
    "\t\t# train_epoch_losses = [train_batch(model, optimizer, loss_fn, x=x.to(device), y=y.to(device)) for x, y in train_dataloader]\n",
    "\t\ttrain_epoch_loss = np.mean(train_epoch_losses)\n",
    "\n",
    "\t\t\n",
    "\t\t# train_epoch_accuracies = [accuracy(model, x=x.to(device), y=y.to(device)) for x, y in train_dataloader]\n",
    "\t\ttrain_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "\n",
    "\t\ttrain_losses.append(train_epoch_loss)\n",
    "\t\ttrain_accuracies.append(train_epoch_accuracy)\n",
    "\t\n",
    "\tend_time = timeit.default_timer()\n",
    "\tfull_training_time = end_time - start_time\n",
    "\t\n",
    "\treturn train_losses, train_accuracies, full_training_time\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 of 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloaders)):\n\u001b[0;32m---> 29\u001b[0m \ttrain_losses, train_accuracies, train_time \u001b[38;5;241m=\u001b[39m \u001b[43mperform_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \tn_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     31\u001b[0m \t\u001b[38;5;66;03m# After training is complete see model accuarcy \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 38\u001b[0m, in \u001b[0;36mperform_training\u001b[0;34m(model, train_dataloader)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     37\u001b[0m \tx,y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m \ttrain_epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \ttrain_epoch_accuracies \u001b[38;5;241m=\u001b[39m accuracy(model, x, y)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# train_epoch_loss = \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# train_epoch_losses = [train_batch(model, optimizer, loss_fn, x=x.to(device), y=y.to(device)) for x, y in train_dataloader]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 12\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(model, opt, loss_fn, x, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss_fn(model(x\u001b[38;5;241m.\u001b[39mto(device)), y\u001b[38;5;241m.\u001b[39mto(device)) \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[1;32m     11\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Make a GD step\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [vgg11_model, vgg19_model]\n",
    "train_dataloaders = [vgg11_train_loader, vgg19_train_loader]\n",
    "test_dataloaders = [vgg11_test_loader, vgg19_test_loader]\n",
    "\n",
    "for model in models:\n",
    "\tmodel = model.to(device)\n",
    "\t# better_summary(model.to(device), (3, 224, 224))\n",
    "\t# print(model)\n",
    "\tfor param in model.features.parameters():\n",
    "\t\tparam.requires_grad = False # Freezing these layers\n",
    "\n",
    "\tmodel.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "\tmodel.classifier = nn.Sequential(nn.Flatten(),\n",
    "\t\t\t\t\t\t\t\t  \tnn.Linear(512, 128),\n",
    "\t\t\t\t\t\t\t\t\tnn.ReLU(), # Activation between linear layers\n",
    "\t\t\t\t\t\t\t\t\tnn.Dropout(0.2),\n",
    "\t\t\t\t\t\t\t\t \tnn.Linear(128, 10)\n",
    "\t\t\t\t\t\t\t\t).to(device)\n",
    "\t\n",
    "\t# better_summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "for model in models:\n",
    "\t# Ensure on same device\n",
    "\tmodel = model.to(device)\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\tfor x in range(len(train_dataloaders)):\n",
    "\t\ttrain_losses, train_accuracies, train_time = perform_training(model, train_dataloaders[x])\n",
    "\t\tn_epochs = 5\n",
    "\t\t# After training is complete see model accuarcy \n",
    "\t\tmodel.eval() \n",
    "\t\ttest_accuracies = []\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor x, y in test_dataloaders[x]:\n",
    "\t\t\t\tx, y = x.to(device), y.to(device)\n",
    "\t\t\t\tacc = accuracy(model, x, y)\n",
    "\t\t\t\ttest_accuracies.append(acc)\n",
    "\t\ttest_accuracy = np.mean(test_accuracies)\n",
    "\t\tprint(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
    "\t\tplt.figure(figsize=(13,3))\n",
    "\t\tplt.subplot(121)\n",
    "\t\tplt.title('Training Loss value over epochs')\n",
    "\t\tplt.plot(np.arange(n_epochs) + 1, train_losses)\n",
    "\t\tplt.subplot(122)\n",
    "\t\tplt.title('Testing Accuracy value over epochs')\n",
    "\t\tplt.plot(np.arange(n_epochs) + 1, train_accuracies)\n",
    "\n",
    "\n",
    "for loss in train_losses:\n",
    "\tprint(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features\n",
      "avgpool\n",
      "classifier\n",
      "Running epoch 1 of 5\n",
      "Running epoch 2 of 5\n",
      "Running epoch 3 of 5\n"
     ]
    }
   ],
   "source": [
    "# Resnet\n",
    "resnet_models = [resnet18_model, resnet152_model]\n",
    "resnet_trainloads = [resnet18_train_loader, resnet152_train_loader]\n",
    "res_test = [resnet18_test_loader, resnet152_test_loader]\n",
    "\n",
    "\n",
    "for curr_model in resnet_models:\n",
    "\n",
    "\tcurr_model = curr_model.to(device)\n",
    "\tfor name, layer in model.named_children():\n",
    "\t\tprint(name)\n",
    "\n",
    "\t\tfor param in layer.parameters():\n",
    "\t\t\tparam.requires_grad = False\n",
    "\t\tif name == \"avgpool\":\n",
    "\t\t\tlayer = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "\t\t# Accessing the fully connected layer \n",
    "\t\tinput_features = curr_model.fc.in_features\n",
    "\t\tif name == 'fc':\n",
    "\t\t\tlayer = nn.Sequential(nn.Flatten(),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(input_features, 128),\n",
    "\t\t\t\t\t\t\t\t\tnn.ReLU(), # Activation between linear layers\n",
    "\t\t\t\t\t\t\t\t\tnn.Dropout(0.2),\n",
    "\t\t\t\t\t\t\t\t\tnn.Linear(128, 10)\n",
    "\t\t\t\t\t\t\t\t).to(device)\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t# print \n",
    "\n",
    "\t\t# model.fc = nn.Linear(num_features, 2)\n",
    "\t\n",
    "\t\t\n",
    "# print(better_summary(curr_model,(3, 224, 224)))\n",
    "\n",
    "\toptimizer = torch.optim.Adam(curr_model.parameters(), lr=1e-3)\n",
    "\tfor x in range(len(resnet_trainloads)):\n",
    "\t\ttrain_losses, train_accuracies, train_time = perform_training(curr_model, resnet_trainloads[x])\n",
    "\t\tn_epochs = 5\n",
    "\n",
    "\t\t# After training is complete see model accuarcy \n",
    "\t\tcurr_model.eval() \n",
    "\t\ttest_accuracies = []\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor x, y in res_test[x]:\n",
    "\t\t\t\tx, y = x.to(device), y.to(device)\n",
    "\t\t\t\tacc = accuracy(curr_model, x, y)\n",
    "\t\t\t\ttest_accuracies.append(acc)\n",
    "\t\ttest_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "\t\t\n",
    "\t\tprint(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
    "\t\tplt.figure(figsize=(13,3))\n",
    "\t\tplt.subplot(121)\n",
    "\t\tplt.title('Training Loss value over epochs')\n",
    "\t\tplt.plot(np.arange(n_epochs) + 1, train_losses)\n",
    "\t\tplt.subplot(122)\n",
    "\t\tplt.title('Testing Accuracy value over epochs')\n",
    "\t\tplt.plot(np.arange(n_epochs) + 1, train_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Images\n",
    "<p>In this section we test our trained models on images from Google Images\n",
    "from 3 of CIFAR10's classes: airplane, cat, and frog<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
