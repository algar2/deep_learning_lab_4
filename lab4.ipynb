{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torch.nn import ReLU\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# There were issues with my certificate\n",
    "import ssl\n",
    "import certifi\n",
    "\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Data Processing\n",
    "Here we load the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "# batch_size = 4 # In the powerpoint slides it is 32?\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #functions to show an image\n",
    "# def imshow(img):\n",
    "# \timg = img / 2 + 0.5 # unnormalize\n",
    "# \tnpimg = img.numpy()\n",
    "# \tplt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "# \tplt.show()\n",
    "\n",
    "\n",
    "# #get random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# # print labels\n",
    "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "\tif torch.cuda.is_available():\n",
    "\t\treturn torch.device ('cuda' )\n",
    "\telif torch.backends.mps.is_available():\n",
    "\t\treturn torch.device ('mps')\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')\n",
    "\t\n",
    "device = get_device()\t\n",
    "\n",
    "\n",
    "def preprocess_data(transformations, batch_size, subset_size=24000):\n",
    "\t\"\"\"\n",
    "\tThis function creates dataloaders with model-specific transformations for data preprocessing\n",
    "\t\"\"\"\n",
    "\t# Load the full training dataset\n",
    "\tfull_train_set = torchvision.datasets.CIFAR10(\n",
    "\t\troot='./data', \n",
    "\t\ttrain=True, \n",
    "\t\tdownload=True, \n",
    "\t\ttransform=transformations\n",
    "\t)\t\n",
    "\n",
    "\t# Creating a subset of the training data\n",
    "\tif subset_size and subset_size < len(full_train_set):\n",
    "\t\t# Creates a random subset\n",
    "\t\tindices = torch.randperm(len(full_train_set))[:subset_size]\n",
    "\t\ttrain_dataset = Subset(full_train_set,indices)\n",
    "\telse:\n",
    "\t\ttrain_dataset = full_train_set\n",
    "\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\ttest_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
    "\ttest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\t# Should we return train_set and test_loader too?\n",
    "\treturn train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our CNNs\n",
    "***\n",
    "We will be experimenting with various pre-trained Deep CNNs - specifically ResNet and VGG architectures.\n",
    "\n",
    "We will be using 2 different versions of Resnet and 2 different versions of VGG.\n",
    "\n",
    "VGG: We will be using models `vgg11` , `vgg11_bn` (with batch normalization), `vgg19`, `vgg19_bn`\n",
    "\n",
    "RESNET: We will be using `resnet18` (ResNet-18) and `resnet152` (ResNet-152)\n",
    "\n",
    "** apply its specific preprocessing transformations to  CIFAR-10 dataset when creating DataLoader objects. **\n",
    "\n",
    "For each model we use, we will:\n",
    "1. Import, insantiate, and load it with default, pretrained weights\n",
    "2. Get the correct preprocessing transformations for that specific model\n",
    "3. Apply those transformations to our CIFAR10 dataset\n",
    "4. Modify the classifer part of the model for our 10-class classification task.\n",
    "-> this means that we need to freeze the ConvLayer(s) parts of the network, and modify the last couple layers -- follow Lecture 9 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "            Conv2d-3         [32, 64, 224, 224]          36,928\n",
      "              ReLU-4         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [32, 64, 112, 112]               0\n",
      "            Conv2d-6        [32, 128, 112, 112]          73,856\n",
      "              ReLU-7        [32, 128, 112, 112]               0\n",
      "            Conv2d-8        [32, 128, 112, 112]         147,584\n",
      "              ReLU-9        [32, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [32, 128, 56, 56]               0\n",
      "           Conv2d-11          [32, 256, 56, 56]         295,168\n",
      "             ReLU-12          [32, 256, 56, 56]               0\n",
      "           Conv2d-13          [32, 256, 56, 56]         590,080\n",
      "             ReLU-14          [32, 256, 56, 56]               0\n",
      "           Conv2d-15          [32, 256, 56, 56]         590,080\n",
      "             ReLU-16          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [32, 256, 28, 28]               0\n",
      "           Conv2d-18          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [32, 512, 28, 28]               0\n",
      "           Conv2d-20          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [32, 512, 28, 28]               0\n",
      "           Conv2d-22          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [32, 512, 14, 14]               0\n",
      "           Conv2d-25          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [32, 512, 14, 14]               0\n",
      "           Conv2d-27          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [32, 512, 14, 14]               0\n",
      "           Conv2d-29          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [32, 512, 7, 7]               0\n",
      "           Linear-33                 [32, 4096]     102,764,544\n",
      "             ReLU-34                 [32, 4096]               0\n",
      "          Dropout-35                 [32, 4096]               0\n",
      "           Linear-36                 [32, 4096]      16,781,312\n",
      "             ReLU-37                 [32, 4096]               0\n",
      "          Dropout-38                 [32, 4096]               0\n",
      "           Linear-39                 [32, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 7000.99\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 7547.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16().to(device)\n",
    "\n",
    "\n",
    "# summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "def better_summary(model, input_size, batch_size=32):\n",
    "\t\"\"\"\n",
    "\tA better summary function that ensures device consistency between weights used for the VGG model\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Get the device the model is on\n",
    "\tmodel_on_cpu = model.to('cpu')\n",
    "\tsummary(model_on_cpu, input_size, batch_size=batch_size)\n",
    "\t# Move model back to original device\n",
    "\tmodel.to(device)\n",
    "\n",
    "better_summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [32, 64, 112, 112]               0\n",
      "            Conv2d-4        [32, 128, 112, 112]          73,856\n",
      "              ReLU-5        [32, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [32, 128, 56, 56]               0\n",
      "            Conv2d-7          [32, 256, 56, 56]         295,168\n",
      "              ReLU-8          [32, 256, 56, 56]               0\n",
      "            Conv2d-9          [32, 256, 56, 56]         590,080\n",
      "             ReLU-10          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [32, 256, 28, 28]               0\n",
      "           Conv2d-12          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [32, 512, 28, 28]               0\n",
      "           Conv2d-14          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [32, 512, 14, 14]               0\n",
      "           Conv2d-17          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [32, 512, 14, 14]               0\n",
      "           Conv2d-19          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [32, 512, 7, 7]               0\n",
      "           Linear-23                 [32, 4096]     102,764,544\n",
      "             ReLU-24                 [32, 4096]               0\n",
      "          Dropout-25                 [32, 4096]               0\n",
      "           Linear-26                 [32, 4096]      16,781,312\n",
      "             ReLU-27                 [32, 4096]               0\n",
      "          Dropout-28                 [32, 4096]               0\n",
      "           Linear-29                 [32, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 132,863,336\n",
      "Trainable params: 132,863,336\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 4011.99\n",
      "Params size (MB): 506.83\n",
      "Estimated Total Size (MB): 4537.20\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [32, 64, 112, 112]               0\n",
      "            Conv2d-4        [32, 128, 112, 112]          73,856\n",
      "              ReLU-5        [32, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [32, 128, 56, 56]               0\n",
      "            Conv2d-7          [32, 256, 56, 56]         295,168\n",
      "              ReLU-8          [32, 256, 56, 56]               0\n",
      "            Conv2d-9          [32, 256, 56, 56]         590,080\n",
      "             ReLU-10          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [32, 256, 28, 28]               0\n",
      "           Conv2d-12          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [32, 512, 28, 28]               0\n",
      "           Conv2d-14          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [32, 512, 14, 14]               0\n",
      "           Conv2d-17          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [32, 512, 14, 14]               0\n",
      "           Conv2d-19          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [32, 512, 1, 1]               0\n",
      "          Flatten-23                  [32, 512]               0\n",
      "           Linear-24                  [32, 128]          65,664\n",
      "             ReLU-25                  [32, 128]               0\n",
      "          Dropout-26                  [32, 128]               0\n",
      "           Linear-27                   [32, 10]           1,290\n",
      "================================================================\n",
      "Total params: 9,287,434\n",
      "Trainable params: 66,954\n",
      "Non-trainable params: 9,220,480\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 3999.97\n",
      "Params size (MB): 35.43\n",
      "Estimated Total Size (MB): 4053.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18, resnet152, ResNet18_Weights, ResNet152_Weights\n",
    "from torchvision.models import vgg11, vgg11_bn, vgg19, vgg19_bn, VGG11_Weights, VGG19_Weights\n",
    "\n",
    "batch_size = 32\n",
    "# Load ResNet models with pretrained weights\n",
    "resnet18_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "resnet152_model = resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "\n",
    "# Load VGG models with pretrained weights for transfer learning\n",
    "vgg11_model = vgg11(weights=VGG11_Weights.DEFAULT)\n",
    "vgg19_model = vgg19(weights=VGG19_Weights.DEFAULT)\n",
    "\n",
    "# Get transformations for every model\n",
    "resnet18_transforms = ResNet18_Weights.DEFAULT.transforms()\n",
    "resnet152_transforms = ResNet152_Weights.DEFAULT.transforms()\n",
    "vgg11_transforms = VGG11_Weights.DEFAULT.transforms()\n",
    "vgg19_transforms = VGG19_Weights.DEFAULT.transforms()\n",
    "\n",
    "models = [vgg11_model]\n",
    "\n",
    "\n",
    "# Create dataloaders for each model with its specific transforms\n",
    "resnet18_train_loader, resnet18_test_loader = preprocess_data(resnet18_transforms, batch_size=batch_size)\n",
    "resnet152_train_loader, resnet152_test_loader = preprocess_data(resnet152_transforms, batch_size=batch_size)\n",
    "vgg11_train_loader, vgg11_test_loader = preprocess_data(vgg11_transforms, batch_size=batch_size)\n",
    "vgg19_train_loader, vgg19_test_loader = preprocess_data(vgg19_transforms, batch_size=batch_size) \n",
    "list_of_transformations = [resnet18_transforms, resnet152_transforms, vgg11_transforms, vgg19_transforms]\n",
    "\n",
    "for model in models:\n",
    "\tbetter_summary(model.to(device), (3, 224, 224))\n",
    "\t# print(model)\n",
    "\tfor param in model.features.parameters():\n",
    "\t\tparam.requires_grad = False # Freezing these layers\n",
    "\n",
    "\tmodel.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "\tmodel.classifier = nn.Sequential(nn.Flatten(),\n",
    "\t\t\t\t\t\t\t\t  \tnn.Linear(512, 128),\n",
    "\t\t\t\t\t\t\t\t\tnn.ReLU(), # Activation between linear layers\n",
    "\t\t\t\t\t\t\t\t\tnn.Dropout(0.2),\n",
    "\t\t\t\t\t\t\t\t \tnn.Linear(128, 10))\n",
    "\t\n",
    "\tbetter_summary(model.to(device), (3, 224, 224))\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Experimentation and Transfer Learning\n",
    "\n",
    "Doing transfer learning on ResNet is slightly different from VGG (the layers don't have the same names), so we print the networks to know which layers to freeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_batch(model, opt, loss_fn, x, y):\n",
    "\tmodel.train()\n",
    "\t\n",
    "\topt.zero_grad()\n",
    "\tbatch_loss = loss_fn(model(x), y) # Loss\n",
    "\tbatch_loss.backward() # Compute gradients\n",
    "\topt.step() # Make a GD step\n",
    "\n",
    "\treturn batch_loss.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def accuracy(model, x, y,):\n",
    "\tmodel.eval()\n",
    "\tprediction = model(x)\n",
    "\targmaxes = prediction.argmax(dim=1)\n",
    "\ts = torch.sum((argmaxes == y).float()/len(y))\n",
    "\treturn s.cpu().numpy()\n",
    "\n",
    "def perform_training(dataloader=vgg11_train_loader):\n",
    "\t\"\"\"\n",
    "\tTrain the model and evaluate on test set after each epoch.\n",
    "\t\"\"\"\n",
    "\ttrain_losses, train_accuracies, n_epochs = [], [], 5\n",
    "\n",
    "\tstart_time = timeit.default_timer()\n",
    "\tfor epoch in range(n_epochs):\n",
    "\t\tprint(f\"Running epoch {epoch + 1} of {n_epochs}\")\n",
    "\n",
    "\t\ttrain_epoch_losses, train_epoch_accuracies = [], []\n",
    "\t\t\n",
    "\t\t# for x, y in dataloader:\n",
    "\t\t# \tx,y = x.to(device), y.to(device)\n",
    "\t\t# \tloss = train_batch(model, optimizer, loss_fn, x, y)\n",
    "\t\t# \ttrain_epoch_losses\n",
    "\t\ttrain_epoch_losses = [train_batch(model, optimizer, loss_fn, x=x.to(device), y=y.to(device)) for x, y in vgg11_train_loader]\n",
    "\t\ttrain_epoch_loss = np.mean(train_epoch_losses)\n",
    "\n",
    "\t\ttrain_epoch_accuracies = [accuracy(model, x=x.to(device), y=y.to(device)) for x, y in vgg11_train_loader]\n",
    "\t\ttrain_epoch_accuracy = np.mean(train_epoch_accuracies)\n",
    "\n",
    "\t\ttrain_losses.append(train_epoch_loss)\n",
    "\t\ttrain_accuracies.append(train_epoch_accuracy)\n",
    "\t\n",
    "\tend_time = timeit.default_timer()\n",
    "\tfull_training_time = end_time - start_time\n",
    "\t\n",
    "\treturn train_losses, train_accuracies\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1 of 5\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses, train_accuracies = perform_training(models[0])\n",
    "n_epochs = 5\n",
    "\n",
    "curr_model = models[0] \n",
    "\n",
    "# After training is complete\n",
    "curr_model.eval() \n",
    "\n",
    "test_accuracies = []\n",
    "with torch.no_grad():\n",
    "\tfor x, y in vgg11_test_loader:\n",
    "\t\tx, y = x.to(device), y.to(device)\n",
    "\t\tacc = accuracy(model, x, y)\n",
    "\t\ttest_accuracies.append(acc)\n",
    "test_accuracy = np.mean(test_accuracies)\n",
    "print(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "for loss in train_losses:\n",
    "\tprint(loss)\n",
    "\n",
    "plt.figure(figsize=(13,3))\n",
    "plt.subplot(121)\n",
    "plt.title('Training Loss value over epochs')\n",
    "plt.plot(np.arange(n_epochs) + 1, train_losses)\n",
    "plt.subplot(122)\n",
    "plt.title('Testing Accuracy value over epochs')\n",
    "plt.plot(np.arange(n_epochs) + 1, train_accuracies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
